---
title: "Trabalho Final Finalcial analytics"
authors:
  - name: Michel
  - name: Hélio
  - name: Renan
  - name: Tiago
format:
  html:
    code-fold: true
execute:
  freeze: true
  warning: false
---

# Bibliotecas

```{r}
#| output: false
library(ggplot2)
library(fpp3)
library(rugarch)
library(tsibble)
library(yfR)
library(zoo)
library(xts)
library(lubridate)
library(patchwork)
library(lmtest)
library(fGarch)
```

## Pré Procesamento 
Seleção das Ações/Ativos e data de ínicio da série 
Escolhemos ínicio de 2022 para escapar dos impactos da pandemia 

```{r}

start_date <- '2021-06-01'

ativos <- c(
  "NVDC34.SA",
  "BCSA34.SA", 
  "AMZO34.SA",
  "RENT3.SA",
  "PRIO3.SA",
  "TASA4.SA"
)
```

Selecionando ações e transformando dataframe em tsbible 

```{r}
#| output: false
da <- yfR::yf_get(
  ativos,
  first_date = start_date,
  last_date = Sys.Date(),
  bench_ticker = "^BVSP",
  type_return = "log",
  freq_data = "daily",
  do_complete_data = TRUE
)

da_tsibble <- da |>
  as_tsibble(key = ticker, index = ref_date, regular = FALSE)

View(da_tsibble)

```

## 1. Plotar gráficos de preço e retorno e analisar possível heterocedasticidade condicional:

## 1.1 Gráfico de preços e dos log retornos

### Preços
```{r}
#Função para formatar legenda
quarter_label <- function(x) {
  paste0(year(x), "Q", quarter(x))
}
```


```{r}
#criando gráfico da série de preços
da_tsibble |>
  autoplot(price_adjusted, colour = "black") +
  facet_wrap(~ticker, scales = "free_y", ncol = 1)+
  scale_x_date(date_breaks = "3 months", label=quarter_label) +
  labs ( title = "Série de Preço", y = "Preço", x = "Data") +
  theme(plot.title = element_text(hjust = 0.5))
```  

### Log Retornos
```{r}
#criando gráfico da série de retornos
da_tsibble |>
  autoplot(ret_adjusted_prices, colour = "black") +
  facet_wrap(~ticker, scales = "free_y", ncol = 1) +
  scale_x_date(date_breaks = "3 months", labels = quarter_label) +
  labs ( title = "Série de Log Retornos", y = "Log Retornos", x = "Data") +
  theme(plot.title = element_text(hjust = 0.5))

```

## 1.2 Analisando possível heterodasticidade condicional

Faremos a transformação da variável de retorno em retorno quadrático, que será a proxi da volatividade dos retornos.

```{r}
da_tsibble |>
  dplyr::mutate(ret2 = ret_closing_prices^2) |>
  autoplot(ret2, colour = "black") +
  facet_wrap(~ticker, ncol = 1)
```

Com a proxi da volatividade, verificaremos a existência de heterodasticidade condicional através dos seguintes métodos:

1.2.1. ACF dos retornos quadráticos

1.2.2. Teste Ljung-Box dos retornos quadráticos

### 1.2.1. ACF dos retornos ao quadrado

```{r}
# Gerando variável dos retornos quadráticos
da_tsibble  <-  da_tsibble |>
  dplyr::mutate(ret2 = ret_closing_prices^2)
```

```{r}
da_tsibble |>
  ACF(ret2) |>
  autoplot()
```

### 1.2.2. Testes Ljung-Box
```{r}

resultados <- data.frame(empresa = character(), p_value = numeric())

for (empresa in ativos) {
  # Filtrar o dataframe para a empresa atual
  da_tsibble_filtered <- da_tsibble |> filter(ticker == empresa)
  
  # Realizar o teste Box-Ljung
  box_test_result <- Box.test(da_tsibble_filtered$ret2, type = "Ljung-Box")
  
  # Adicionar o resultado ao dataframe de resultados
  resultados <- rbind(resultados, 
                               data.frame(empresa = empresa, 
                                          p_value = box_test_result$p.value 
                                          ))
}

print(resultados)
```

Conclusões:

ACF:

As empresas NVDC34.SA, RENT3.SA, PRIO3.SA e TASA4.SA têm p-values maiores que 0.05, indicando que não há evidências suficientes para rejeitar a hipótese nula de que os dados são independentemente distribuídos. Isso sugere que não há autocorrelação nos dados dessas empresas.

Por outro lado, as empresas BCSA34.SA e AMZO34.SA têm p-values menores que 0.05, o que significa que há evidências suficientes para rejeitar a hipótese nula. Isso indica a presença de autocorrelação nos dados dessas empresas.


# 2 Ajustando modelos Arch / Garch

## 2.1. BCSA34.SA

Filtrando a base e selecionando variáveis
```{r} 
# filtrando pela empresa Santander
  santander <- da_tsibble %>%
    dplyr::filter(ticker == 'BCSA34.SA') %>%
    select(ref_date, ret_closing_prices) %>%
    arrange(ymd(ref_date))

# Criando variável dos log retornos
ret <- santander %>%
  slice(-1) %>%
  pull(ret_closing_prices)

#criando variáveis dos log retornos ao quadrado
ret2 <- ret^2

```

Verificando série, ACF e PACF para determinar parâmetros
```{r}
par(mfrow=c(1,2))
acf(ret,60,na.action = na.pass)
acf(ret2,60,na.action = na.pass)
```

Considerando que a série de retornos ao quadrado apresenta autocorrelação relevante até o lag 3, o modelo utilizará o parâmetro m = 3.

Fit do modelo
```{r}
fit_arch_santander_30 = garchFit(~garch(3,0),data=ret ,trace=F)

summary(fit_arch_santander_30)
```

Verificando resíduos
```{r}
residuos  <- residuals(fit_arch_santander_30, standardize=T)
par(mfrow=c(1,2))
ts.plot(residuos)
acf(residuos)
```

O resíduo apresenta aspecto de ruído branco. Não possuo autocorrelação entre os lags, e aparenta distribuição com média 0.

## teste Ljung Box para verificar que os resíduos são RB

```{r}
Box.test(residuos,lag=3,type="Ljung")
```

Uma vez que o resultado do p-valor está acima de 0.05, não rejeitamos a hipótese nula de que há autocorrelação entre os resíduos. Portanto alteraremos os parâmetros da modelagem para avaliação de melhor desempenho do modelo.




















Função para ajustar um garch

```{r}
#| eval: false
#| echo: false
garch_individual <- function(parms, ret, prog = NULL) {
  if (!is.null(prog)) prog()
  # daria para adicionar mais hiperparametros!!!
  garch_model = ugarchspec(
    variance.model = list(
      model = "fGARCH",
      submodel = "GARCH",
      garchOrder = c(parms$m, parms$n)
    ),
    mean.model = list(
      armaOrder = c(parms$p, parms$q),
      include.mean = TRUE
    ),
    distribution.model = parms$dist
  )
  # as vezes ele nao converge
  suppressWarnings({
    fit <- ugarchfit(garch_model, data = da)
  })
  fit
}
```
Função para ajustar uma grid de garchs e pegar as informações

```{r}

### OMITIDO

```













Rodando as funções

```{r}
#| eval: false
#| echo: false
melhores_por_ativo <- ativos |>
  purrr::set_names() |>
  purrr::map(melhor_garch, .progress = TRUE) |>
  dplyr::bind_rows(.id = "ticker")
```




```{r}
#| eval: false
#| echo: false
melhores_por_ativo <- readr::read_rds("melhores_por_ativo.rds")
```


## Prever volatilidade um passo à frente

Função que ajusta o modelo e faz as previsões

```{r}
#| eval: false
#| echo: false
prever_volatilidade <- function(parms, n_steps = 5) {
  usethis::ui_info("Prevendo volatilidade para {parms$ticker}...")

  ret <- da_train |>
    dplyr::filter(ticker == parms$ticker) |>
    pull(ret_closing_prices)

  garch_model = ugarchspec(
    variance.model = list(
      model = "fGARCH",
      submodel = "GARCH",
      garchOrder = c(parms$m, parms$n)
    ),
    mean.model = list(
      armaOrder = c(parms$p, parms$q),
      include.mean = TRUE
    ),
    distribution.model = parms$dist
  )

  fit <- ugarchfit(garch_model, data = ret, out.sample = n_steps - 1)

  if (parms$dist == "std") {
    shape <- as.numeric(fit@fit$coef["shape"])
  } else {
    shape <- NA_real_
  }

  forecasts <- ugarchforecast(fit, n.ahead = n_steps)@forecast
  tibble::tibble(
    ticker = parms$ticker,
    serie = as.numeric(forecasts$seriesFor),
    volatilidade = as.numeric(forecasts$sigmaFor),
    shape = shape
  )
}
```

Ajustando modelos finais e prevendo volatilidade futura

```{r}
#| eval: false
#| echo: false
parametros_melhores <- melhores_por_ativo |>
  group_by(ticker) |>
  slice_head(n = 1) |>
  ungroup()

vol_futuro <- parametros_melhores |>
  group_split(ticker) |>
  purrr::map(\(x) prever_volatilidade(x, n_steps = 5)) |>
  dplyr::bind_rows()

vol_futuro
```

## Comparar volatilidades entre os retornos selecionados

...

## Montagem de portfolio

Reproduzindo código daqui:

<https://www.codingfinance.com/post/2018-05-31-portfolio-opt-in-r/>

Versão em python

<https://www.codingfinance.com/post/2018-05-31-portfolio-opt-in-python/>

```{r}
#| eval: false
#| echo: false
da_wide <- da_train |>
  dplyr::select(ref_date, name = ticker, value = ret_closing_prices) |>
  tidyr::pivot_wider()

da_xts <- da_wide |>
  timetk::tk_xts(select = -ref_date, date_var = ref_date)
```


```{r}
#| eval: false
#| echo: false
mean_ret <- colMeans(da_xts, na.rm = TRUE)
print(round(mean_ret, 5))
```

Next we will calculate the covariance matrix for all these stocks. We will NOT annualize it by multiplying by 252.

```{r}
#| eval: false
#| echo: false
cov_mat <- cov(da_xts, use = "complete.obs")
print(round(cov_mat,6))
```

Before we apply our methods to thousands of random portfolio, let us demonstrate the steps on a single portfolio.

To calculate the portfolio returns and risk (standard deviation) we will us need

- Mean assets returns
- Portfolio weights
- Covariance matrix of all assets
- Random weights

```{r}
#| eval: false
#| echo: false
set.seed(2)
# Calculate the random weights
wts <- runif(n = length(ativos))

(wts <- wts/sum(wts))

# Calculate the portfolio returns
(port_returns <- sum(wts * mean_ret))

# Calculate the portfolio risk
(port_risk <- sqrt(t(wts) %*% (cov_mat %*% wts)))

# Calculate the Sharpe Ratio
(sharpe_ratio <- port_returns/port_risk)
```

We have everything we need to perform our optimization. All we need now is to run this code on 5000 random portfolios. For that we will use a for loop.


~Before we do that, we need to create empty vectors and matrix for storing our values.~

```{r}
#| eval: false
#| echo: false



sim_returns <- function(i) {
  wts <- runif(length(ativos))
  wts <- wts / sum(wts)
  port_ret <- sum(wts * mean_ret)
  port_sd <- as.numeric(sqrt(t(wts) %*% (cov_mat %*% wts)))
  sr <- port_ret / port_sd

  wts |>
    purrr::set_names(ativos) |>
    tibble::enframe() |>
    tidyr::pivot_wider() |>
    dplyr::mutate(
      return = port_ret,
      risk = port_sd,
      sharpe = sr
    )
}

portfolio_values <- purrr::map(1:5000, sim_returns, .progress = TRUE) |>
  bind_rows(.id = "run")

min_var <- portfolio_values[which.min(portfolio_values$risk),]
max_sr <- portfolio_values[which.max(portfolio_values$sharpe),]

```

Lets plot the weights of each portfolio. First with the minimum variance portfolio.

```{r}
#| eval: false
#| echo: false
min_var |>
  pivot_longer(2:6) |>
  mutate(name = forcats::fct_reorder(name, value)) |>
  ggplot(aes(name, value)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Asset",
    y = "Weight",
    title = "Minimum variance portfolio weights"
  )

```

```{r}
#| eval: false
#| echo: false
max_sr |>
  pivot_longer(2:6) |>
  mutate(name = forcats::fct_reorder(name, value)) |>
  ggplot(aes(name, value)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Asset",
    y = "Weight",
    title = "Tangency portfolio weights"
  )
```

```{r}
#| eval: false
#| echo: false
portfolio_values |>
  ggplot(aes(x = risk, y = return, color = sharpe)) +
  geom_point() +
  theme_classic() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = 'Risk',
    y = 'Returns',
    title = "Portfolio Optimization & Efficient Frontier"
  ) +
  geom_point(
    aes(x = risk, y = return),
    data = min_var,
    color = 'red',
    size = 3
  ) +
  geom_point(
    aes(x = risk, y = return),
    data = max_sr,
    color = 'orange',
    size = 3
  )
```

## VaR do portfolio

```{r}
#| eval: false
#| echo: false
pesos_finais <- min_var |>
  dplyr::select(2:6) |>
  as.numeric()


rt_final <- mean(vol_futuro$serie * pesos_finais)
st_dev_final <- sqrt(pesos_finais %*% cov_mat %*% pesos_finais)
nu <- min(vol_futuro$shape)
valor_t <- qt(.95, nu)

(VaR <- rt_final + valor_t * st_dev_final / sqrt(nu/(nu-2)))
```

## CAPM

```{r}
#| eval: false
#| echo: false
portfolio_returns <- da_train |>
  tidyquant::tq_portfolio(
    ticker,
    ret_closing_prices,
    weights = pesos_finais,
    col_rename = "portfolio"
  )

market_returns <- yfR::yf_get(
  "^BVSP",
  first_date = data_corte + 1,
  type_return = "log",
  freq_data = "daily",
  do_complete_data = TRUE
) |>
  dplyr::select(ref_date, ibov = ret_closing_prices)

all_returns <- market_returns |>
  dplyr::inner_join(portfolio_returns, "ref_date") |>
  tidyr::drop_na()

(beta_geral <- with(all_returns, cov(portfolio, ibov) / var(ibov)))

calcular_beta <- function(ativo) {
  da_train |>
    dplyr::filter(ticker == ativo) |>
    dplyr::inner_join(market_returns, "ref_date") |>
    tidyr::drop_na() |>
    with(cov(ret_closing_prices, ibov) / var(ibov))
}

betas <- purrr::map_dbl(ativos, calcular_beta) |>
  purrr::set_names(ativos)

sum(betas * pesos_finais)
beta_geral

```

```{r}
#| eval: false
#| echo: false

capm_lm_tudo <- lm(portfolio ~ ibov, data = all_returns) |>
  broom::tidy() |>
  dplyr::filter(term == "ibov") |>
  with(estimate)

capm_lm_individual <- purrr::map_dbl(ativos, \(ativo) {
  da_model <- da_train |>
    dplyr::filter(ticker == ativo) |>
    dplyr::inner_join(market_returns, "ref_date")
  lm(ret_closing_prices ~ ibov, data = da_model) |>
    broom::tidy() |>
    dplyr::filter(term == "ibov") |>
    dplyr::pull(estimate)
}) |>
  purrr::set_names(ativos)


capm_lm_tudo

capm_lm_individual
```